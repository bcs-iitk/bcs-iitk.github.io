<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="">
<!-- <link rel="icon" href="../../../../favicon.ico"> -->

<title>
  Facial Expression Recognition | BCS @IITK
</title>
<link rel="shortcut icon" href="https://bcs-iitk.github.io/pages/bcs-iitk/assets/img/favicon.png">

<!-- Bootstrap core CSS -->
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">

<!-- Custom styles for this template -->
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
<link rel="stylesheet" href="/pages/bcs-iitk/assets/css/main.css">

  </head>
  <body>
    <div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = 'https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v3.1';
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>

<nav class="shadow-sm navbar navbar-expand-md navbar-light fixed-top " style="background-color: #ffffff;">
  <a class="navbar-brand" href="https://bcs-iitk.github.io/pages/bcs-iitk/" style="text-transform: uppercase;">
    <!-- <img src="/assets/img/favicon.png" width="27" height="27" class="rounded-circle d-inline-block align-top" alt=""> -->
     BCS @IITK
  </a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarsExampleDefault" aria-controls="navbarsExampleDefault" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="navbarsExampleDefault">
    <ul class="navbar-nav  mr-auto">
      <li class="nav-item">
        
          <a class="nav-link" href="https://bcs-iitk.github.io">Home</a>
        
      </li>
      <li class="nav-item">
        
          <a class="nav-link" href="https://bcs-iitk.github.io/projects">Projects</a>
        
      </li>
      <li class="nav-item">
        
          <a class="nav-link" href="https://bcs-iitk.github.io/blog-post">Blogs</a>
        
      </li>
      <!-- <li class="nav-item">
        <a class="nav-link" href="https://bcs-iitk.github.io/publications">Publications</a>
      </li> -->
      <li class="nav-item">
        
          <a class="nav-link" href="https://bcs-iitk.github.io/journal-club">Journal Club</a>
        
      </li>
      <li class="nav-item">
        
          <a class="nav-link" href="https://bcs-iitk.github.io/events">Events</a>
        
      </li>
      <li class="nav-item">
        
          <a class="nav-link" href="https://bcs-iitk.github.io/people">People</a>
        
      </li>
      <li class="nav-item">
        
          <a class="nav-link" href="https://bcs-iitk.github.io/gallery">Gallery</a>
        
      </li>
    </ul>

    <!-- <a class="nav-link" href="https://bcs-iitk.github.io/team">Contact</a> -->
    <a class="nav-link" href="https://groups.google.com/forum/#!forum/bcs-iitk-mailing-list" target="_blank">Join Mailing List</a>

  </div>
</nav>


<main role="main" class="container">
  <div class="hero">
  <h1 class="display-5">Facial Expression Recognition</h1>
  <h4 class=""><small class="text-muted">July 1, 2020</small></h4>
  <h4>
    <a href="/pages/bcs-iitk/projects/2020/07/01/Comparing-Deep-Neural-Network-Features-With-Psychological-Representations.html" class="post-nav" data-toggle="tooltip" data-placement="left" title="Comparing Deep Neural Network Features With Psychological Representations"><i class="fas fa-arrow-left"></i></a>
    <a href="https://bcs-iitk.github.io/pages/bcs-iitk/projects" class="post-nav"><i class="fas fa-home"></i></a>
    <a href="/pages/bcs-iitk/projects/2020/07/01/Knowledge-Graph-Reasoning-for-Explainable-Recommendation.html" class="post-nav" data-toggle="tooltip" data-placement="right" title="Knowledge Graph Reasoning for Explainable Recommendation"><i class="fas fa-arrow-right"></i></a>
  </h4>
</div>

<div class="post border-top">
  <div class="post-content lead">
    <p>
      
      <p><strong>Mentor:</strong> Avisha Gaur <br />
<strong>Poster :</strong> <a href="https://drive.google.com/file/d/16-eVFhDVKEf58gj0dlAS7SRuXL24Nvd9/view?usp=sharing" target="_blank">Link</a><br />
<strong>Documentation :</strong> <a href="https://drive.google.com/file/d/1A-Lygswq49kQd_QBPK6TUQI169W5QoRH/view?usp=sharing" target="_blank">Link</a><br /></p>
<h3 id="team-1">Team 1</h3>
<p><strong>Team Members:</strong> Shashi Kumar, Aman Agarwal, Arpit Verma, Gaurav Sharma, Himanshu shetty, Anumam Yadav, and Shakshi. <br /><br />
<strong>Abstract:</strong><br /></p>
<ul>
  <li>Preprocessing : This was the start of the project, basically  we aligned, face - cropped, normalized the images of our dataset and also augmented (flipped and randomly rotated) them to ensure good results.</li>
  <li>CNN model : We implemented a CNN model structure with two convolutional layers and two subsampling layers along with dropouts of 0.5 followed by output dense layer of seven units and softmax activation function.</li>
  <li>Evaluation : We evaluated our model on three cropping methods viz. Cropping with background, Cropping without background, Cropping without forehead. We also made variation in the neuron number of hidden dense layer prior to the output layer as 0, 256, 512, 1024. Further, we performed a ten- fold cross validation on our dataset.</li>
  <li>Results : Finally, We got an average accuracy of 97.49 after performing all above mentioned evaluation statistics.
<br /><br /></li>
</ul>

<h3 id="team-2">Team 2</h3>
<p><strong>Team Members:</strong> Falguni Yadav, Prachi Singh, Pranav Kumar, Saksham Pruthi, Samriddhi Gupta, Tanisha Agrawal, Videeta Sharma, Yatharth Gupta <br /><br />
<strong>Abstract:</strong><br />
With the recent development and application of human–computer interaction systems, facial expression recognition (FER) has become a popular research area. The recognition of facial expression is a difficult problem for existing machine learningand deep learning models because that the images can vary in brightness, background, pose, etc. Feature extraction is very important for FER, even a simple algorithm can be very effective if the extracted features are sufficient to be separable. However, deep learning methods automatically extract features so that some useless features can interfere with useful features. For these reasons, FER is still a challenging problem in computer vision.</p>

<p>Facial expressions are one of the most important features to reflect the human emotional state because they convey useful information to the observer. Several deep learning approaches for facial expression recognition were developed in the last decades, particularly the method of CNN.
This project aims to to detect involuntary emotional response occurring simultaneously with conflicting voluntary emotional response and identifying true emotions by building classifiers that categorise facial expressions into 6 universal emotion categories (+1 neutral). We start off by implementing various pre-processing techniques for the images and use a customised face-cropping method. We also implement Linear Binary Patterns for Feature Extraction and then test these on 2 customised CNN models while measuring their performance.
<br /><br /></p>

<h3 id="team-3">Team 3</h3>
<p><strong>Team Members:</strong> Aditya Prakash, Ananya Gupta, Bhavesh Jain, Shivanshu Tyagi, Urbi Ghosh <br /><br />
<strong>Abstract:</strong><br />
Facial expression recognition has gained a lot of attention in the past few years due to its wide applications. FER can be classified into two categories: Macro Expressions and Micro expressions recognition. We focused on macro expression recognition. Methods for emotion recognition  often involve the Facial Action Coding System (FACS) which describes the facial expression using Action Units (AU). An Action Unit is a facial action like ”raising the Inner Brow”. Detecting such landmarks can be hard, as the distance between them differs depending on the person .</p>

<p>The presented approach uses Convolutional Neural Networks special type of Artificial Neural Networks(ANNs).The proposed network has been trained on the FER-2103 Dataset  and evaluated on the CK+ dataset and achieved an accuracy of 95.6%.
After training on image dataset, the problem statement was expanded to video dataset. Approach for the second problem was  changed: a hybrid of C3D and CNN+LSTM network was used. LSTM has memory ability and suits for processing sequences with contexts well. In the end, real time emotion classification using webcam input was incorporated. 
<br /><br /></p>

<h3 id="team-4">Team 4</h3>
<p><strong>Team Members:</strong> Kusum Bunkar, Deepika Meena, Lakshita Mohanty, Pranay Vandanapu, Rishi Dhakar, Bhavy Gandhi, Mukulesh Shinde <br /><br />
<strong>Abstract:</strong><br />
Facial expressions are one of the most important features to reflect the human emotional state and they convey 55% of a communicated message which is more than the part conveyed by the combination of voice and language. FER technique can be used for the development of human–computer interaction systems, such as social robots, visual interactive games, and data-driven animation.<br /> 
We used a new face cropping and image rotation strategy to improve the accuracy and simplify the CNN structure. By facial cropping we removed the emotionally inactive part of the region and random rotations to cope up with data scarcity. Also Histogram equalization, Z-score normalization, and down-sampling were applied to standardize the image data.The expanded training data thus obtained is used to train the CNN, and we got our best CNN model by ten-fold cross validation. During the validation or testing phase, the normalized testing images (without expansion) were sent to the CNN model from the training phase for prediction. Further we have used the combined datasets to train a model for detecting single and multiple faces and their six different expressions in a realtime environment.
<br /><br /></p>

<h3 id="team-5">Team 5</h3>
<p><strong>Team Members:</strong> Aditya Jindal, Mrigya Gupta, Sampada Sinha, Shruthi Sureshkumar <br /><br />
<strong>Abstract:</strong><br />
Emotions play a hugely important role in our interpersonal communications. Recognizing emotions accurately has wide ranging applications - from marketing to psychology to gaming. In this project we aimed to recognize facial expressions from input image data. We began with a review of existing literature to understand the problem of facial expression recognition. We ended up implementing 3 different papers to obtain better accuracies, and obtained the highest accuracy on the final paper implemented. We were introduced to machine learning, and deep learning techniques. We tested and trained the model on existing standard datasets, and also tested the model with a live webcam feed.<br /></p>

    </p>
  </div>
</div>

<!-- <div class="fb-comments" data-href="https://bcs-iitk.github.io/projects/2020/07/01/Facial-Expression-Recognition.html" data-width="100%" data-numposts="5"></div> -->

</main>

<footer class="footer bg-light">
  <div class="container">
    <div class="footer-content">
      <h1>
        <a href="mailto:bcs.iitk@gmail.com" class="social-icon" target="_blank"><i class="fas fa-envelope"></i></a>
        <a href="https://www.facebook.com/bcs.iitk" class="social-icon" target="_blank"><i class="fab fa-facebook-f"></i></a>
        <a href="https://github.com/bcs-iitk" class="social-icon" target="_blank"><i class="fab fa-github-alt"></i></a>
        <a href="https://youtube.com/channel/UC3vBBqgKjAfRZQ0di-Oy6yQ" class="social-icon" target="_blank"><i class="fab fa-youtube"></i></i></a>
        <a href="https://groups.google.com/forum/#!forum/bcs-iitk-mailing-list" class="social-icon" target="_blank"><i class="fab fa-google-plus-g"></i></a>
        <a href="https://twitter.com/bcs_IITK" class="social-icon" target="_blank"><i class="fab fa-twitter"></i></a>
        <a href="https://linkedin.com/company/brain-and-cognitive-society-iit-kanpur" class="social-icon" target="_blank"><i class="fab fa-linkedin"></i></a>
      </h1>
      <p class="" style="padding-top: 1rem;"><i class="far fa-copyright"></i> 2021 Brain & Cognitive Society, IIT Kanpur</p>
      <p>Designed by <a href="http://shashikg.github.io/" target="_blank" class="text-decoration-none">Shashi Kant</a></p>
    </div>
  </div>
</footer>

<!-- Optional JavaScript -->
<!-- jQuery first, then Popper.js, then Bootstrap JS -->
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>


  </body>
</html>
